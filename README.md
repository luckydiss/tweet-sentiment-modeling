# tweet-sentiment-modeling
# Классификация твитов о катастрофахна основе датасета Kaggle: Natural Language Processing with Disaster Tweets
(https://www.kaggle.com/c/nlp-getting-started)

**Постановка задачи.** Необходимо классифицировать описывает ли твит реальную катастрофу/происшествие (disaster) или не относящуюся к ним (non-disaster)

**Даны следующие признаки:**
*   `id` — идентификатор твита.
*   `text` — текст твита.
*   `keyword` — ключевое слово из твита.
*   `location` — локация, откуда отправлен твит.
*   `target` — целевая переменная (1 — катастрофа, 0 — нет).


### EDA (eda.ipynb):

*   **Баланс классов:** Распределение таргета относительно сбалансированное, но не идеальное: 57% (класс 0) против 43% (класс 1). Балансировка данных в дальнейшем не производилась.
*   **Пропуски:** В столбце `location` отсутствует около 33% данных, в `keyword` — менее 1%. Пропуски в `keyword` были заполнены пустыми строками, `location` использовался для создания нового бинарного признака (1/0).
*   **Анализ длины текста:** Твиты о реальных катастрофах в среднем длиннее и содержат больше слов, чем обычные твиты. Однако распределения сильно перекрываются.
*   **Специальные символы (Feature Engineering):**
    *   **URLs:** Твиты с катастрофами гораздо чаще содержат ссылки (корреляция с таргетом 0.19).
    *   **Mentions (@):** Обычные твиты чаще содержат упоминания пользователей (корреляция -0.10).
    *   **Numbers:** Наличие чисел обычно характеризует описание происшествий (количество жертв, время, дата и т.п.).
*   **Анализ Keywords и Location:**
    *   Найдены ключевые слова с очень высокой вероятностью катастрофы: `outbreak`, `evacuate`, `suicide bomber`, `typhoon`.
    *   Слова с низкой вероятностью катастрофы (часто используются в переносном смысле): `body bags`, `armageddon`, `panic`.
    *   Локации вроде `Mumbai`, `Nigeria` имеют высокий процент disaster-твитов, тогда как `New York` — низкий.
*   **N-граммы и семантика:**
    *   Disaster-твиты содержат конкретику: `fire`, `killed`, `suicide`, `hiroshima`, `california`.
    *   Non-disaster твиты выражают эмоции: `love`, `like`, `want`, `can't`.
    *   Важность контекста: слово `fire` встречается в обоих классах, поэтому униграмм недостаточно, важны биграммы.

### Baseline (baseline.ipynb):

Были построены следующие модели `LogisticRegression`, `Decision Tree`, `Random Forest`, `LGBMClassifier`, `DummyClassifier` с 5 кратной кросс валидацией

**Эксперименты и результаты:**

*   **Baseline (CountVectorizer):** Использовались униграммы и биграммы на сыром тексте. Лучший результат показала лог. регрессия (F1: ~0.792). Деревья решений и Random Forest переобучались на тех. артефактах (например, токен http).
*   **CountVectorizer + удаление стоп-слов:** Попытка убрать стоп-слова не дала прироста (F1 снизился до ~0.791). Следовательно, предлоги и союзы важны для понимания контекста твитов.
*   **CountVectorizer + глубокая очистка:** Была произведена очистка текста: приведение к нижнему регистру, удаление ссылок, упоминаний (@user) и хэштегов. Интерпретируемость модели улучшилась (исчезли мусорные токены), но метрики не выросли (F1 ~0.790), так как наличие ссылок само по себе является маркером катастрофы.
*   **CountVectorizer + ANOVA (Top-1000):** Отбор 1000 лучших признаков через F-тест ускорил обучение в разы, но привел к заметному падению качества (F1 упал до ~0.771) из-за потери редких, но информативных слов.
*   **TF-IDF (Raw Text):** Взвешивание токенов вместо простого подсчета на сырых данных дало наилучший результат (лог. регрессия (ROC AUC: 0.8625, F1 Score: 0.7975)) среди всех экспериментов.
*   **TF-IDF + удаление стоп-слов:** Комбинация взвешивания и удаления стоп-слов сработала хуже, чем чистый TF-IDF (F1 снизился до ~0.7936).
*   **TF-IDF + глубокая очистка:** Применение TF-IDF после очистки твитов не дала лучший результат, чем подход на сырых данных (F1 ~0.7916) - структурные элементы твита (ссылки, спецсимволы) несут полезный сигнал.
*   **TF-IDF + ANOVA (Top-1000):** Самый быстрый вариант обучения, но с наихудшим качеством среди экспериментов с TF-IDF (F1 упал до ~0.760).


###  Нейросетевые подходы (neural_networks.ipynb):

Были построены следующие модели `BiRNN`, `BiGRU`, `BiLSTM`, `CNN + GloVe Twitter`, `BERTweet` с 5 кратной кросс валидацией

**Эксперименты и результаты (Deep Learning):**

1.  **BiRNN (Vanilla):** Результат оказался крайне нестабильным: в лучшем случае (fold 2) f1 достигал ~0.69, однако в остальных случаях наблюдалось резкое переобучение.
2.  **BiGRU/BiLSTM** Показали более стабильную сходимость по сравнению с RNN (F1 ~0.73-0.75), но не смогли превзойти бейзлан
3.  **CNN + GloVe Twitter:** Использование CNN с предобученными эмбеддингами (GloVe 100d). Модель показала результат, близкий к бейзлайну (F1 ~0.78-0.79).
4.  **BERTweet:** Fine-tuning предобученной на англоязычных твитах, модели `vinai/bertweet-base`. Этот подход оказался лучшим среди всех экспериментов (ROC AUC: 0.8961, F1 Score: 0.8126)

**Вывод:**
Таким образом, в экспериментах с классическими методами наилучший результат показал TF-IDF на сыром тексте с лог. регрессией (F1 Score: 0.7975, ROC AUC: 0.8625), при этом очистка данных и отбор признаков чаще снижали качество, указывая на важность структурных элементов твитов - ссылки и стоп-слова. Среди нейросетевых подходов fine-tuning модели BERTweet (vinai/bertweet-base) оказался наилучшим (F1 Score: 0.8126, ROC AUC: 0.8961).
